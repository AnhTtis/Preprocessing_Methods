{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb1d6d5b",
   "metadata": {},
   "source": [
    "# Libraries using for Tabular Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa6449d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "import kagglehub\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3936c27e",
   "metadata": {},
   "source": [
    "## I.Downloading the dataset:\n",
    "\n",
    "In this step, we download the selected tabular dataset directly from **Kaggle** using the `kagglehub` library. This approach ensures that the dataset is always retrieved in its **latest available version**, improving reproducibility and consistency across different environments.\n",
    "\n",
    "After downloading, all dataset files are copied into a predefined project directory structure (`data/tabular/`). This organization helps maintain a clean and standardized layout for data preprocessing experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32b9523c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.13), please consider upgrading to the latest version (0.4.1).\n",
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/kartik2112/fraud-detection?dataset_version_number=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 202M/202M [00:13<00:00, 15.2MB/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset copied to d:\\Data\\Learning\\University\\Year3\\Semester 8\\Data Mining\\Current Semester\\Preprocessing_Methods\\data\\tabular\n"
     ]
    }
   ],
   "source": [
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"kartik2112/fraud-detection\")\n",
    "\n",
    "# Copy files to correct directory\n",
    "destination = \"../data/tabular\"\n",
    "if not os.path.exists(destination):\n",
    "    os.makedirs(destination, exist_ok=True)\n",
    "\n",
    "for file in os.listdir(path):\n",
    "    src = os.path.join(path, file)\n",
    "    dst = os.path.join(destination, file)\n",
    "    shutil.copy2(src, dst)\n",
    "    \n",
    "print(f\"Dataset copied to {os.path.abspath(destination)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27b20ef",
   "metadata": {},
   "source": [
    "After downloading and organizing the dataset, the following files are available in the `data/tabular/` directory:\n",
    "\n",
    "```\n",
    "data/tabular/\n",
    "├── fraudTrain.csv\n",
    "└── fraudTest.csv\n",
    "```\n",
    "\n",
    "### 1. `fraudTrain.csv`\n",
    "\n",
    "This file contains the **training dataset**, which is used to explore the data distribution and perform preprocessing techniques such as handling missing values, normalization, categorical encoding, and feature selection.\n",
    "\n",
    "* Represents historical transaction records used for model training.\n",
    "* Includes both **numerical** and **categorical** attributes related to transactions and customers.\n",
    "* Contains the **target variable** indicating whether a transaction is fraudulent or legitimate.\n",
    "* Used as the primary dataset for analyzing preprocessing effects and feature behavior.\n",
    "\n",
    "### 2. `fraudTest.csv`\n",
    "\n",
    "This file contains the **testing dataset**, which is separated from the training data to simulate unseen data.\n",
    "\n",
    "* Has the same schema and feature structure as `fraudTrain.csv`.\n",
    "* Used to validate preprocessing consistency and evaluate how preprocessing decisions generalize to new data.\n",
    "* Ensures that preprocessing pipelines do not rely on information leakage from the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4d1ac3",
   "metadata": {},
   "source": [
    "## II. Loading the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56b59416",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(os.path.join(destination, \"fraudTrain.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a345cd24",
   "metadata": {},
   "source": [
    "### Analyzing some basic information of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0a0235c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape: (1296675, 23)\n",
      "\n",
      "==================================================\n",
      "Column Names and Data Types:\n",
      "Unnamed: 0                 int64\n",
      "trans_date_trans_time     object\n",
      "cc_num                     int64\n",
      "merchant                  object\n",
      "category                  object\n",
      "amt                      float64\n",
      "first                     object\n",
      "last                      object\n",
      "gender                    object\n",
      "street                    object\n",
      "city                      object\n",
      "state                     object\n",
      "zip                        int64\n",
      "lat                      float64\n",
      "long                     float64\n",
      "city_pop                   int64\n",
      "job                       object\n",
      "dob                       object\n",
      "trans_num                 object\n",
      "unix_time                  int64\n",
      "merch_lat                float64\n",
      "merch_long               float64\n",
      "is_fraud                   int64\n",
      "dtype: object\n",
      "\n",
      "==================================================\n",
      "Missing Values:\n",
      "Unnamed: 0               0\n",
      "trans_date_trans_time    0\n",
      "cc_num                   0\n",
      "merchant                 0\n",
      "category                 0\n",
      "amt                      0\n",
      "first                    0\n",
      "last                     0\n",
      "gender                   0\n",
      "street                   0\n",
      "city                     0\n",
      "state                    0\n",
      "zip                      0\n",
      "lat                      0\n",
      "long                     0\n",
      "city_pop                 0\n",
      "job                      0\n",
      "dob                      0\n",
      "trans_num                0\n",
      "unix_time                0\n",
      "merch_lat                0\n",
      "merch_long               0\n",
      "is_fraud                 0\n",
      "dtype: int64\n",
      "\n",
      "==================================================\n",
      "Numerical Columns (11):\n",
      "['Unnamed: 0', 'cc_num', 'amt', 'zip', 'lat', 'long', 'city_pop', 'unix_time', 'merch_lat', 'merch_long', 'is_fraud']\n",
      "\n",
      "==================================================\n",
      "Categorical Columns (12):\n",
      "['trans_date_trans_time', 'merchant', 'category', 'first', 'last', 'gender', 'street', 'city', 'state', 'job', 'dob', 'trans_num']\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Display basic information about the dataset\n",
    "print(\"Dataset Shape:\", data.shape)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Column Names and Data Types:\")\n",
    "print(data.dtypes)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Check for missing values\n",
    "print(\"Missing Values:\")\n",
    "print(data.isnull().sum())\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# # Basic statistics for numerical columns\n",
    "# print(\"Numerical Statistics:\")\n",
    "# print(data.describe())\n",
    "# print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# # Check class distribution (fraud vs non-fraud)\n",
    "# print(\"Fraud Distribution:\")\n",
    "# print(data['is_fraud'].value_counts())\n",
    "# print(\"\\nFraud Percentage:\")\n",
    "# print(data['is_fraud'].value_counts(normalize=True) * 100)\n",
    "# print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Check numerical columns\n",
    "numerical_cols = data.select_dtypes(include=['int64', 'float64']).columns\n",
    "print(f\"Numerical Columns ({len(numerical_cols)}):\")\n",
    "print(list(numerical_cols))\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Check categorical columns\n",
    "categorical_cols = data.select_dtypes(include=['object']).columns\n",
    "print(f\"Categorical Columns ({len(categorical_cols)}):\")\n",
    "print(list(categorical_cols))\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# # Display unique values for some key categorical columns\n",
    "# key_columns = ['category', 'gender', 'state']\n",
    "# for col in key_columns:\n",
    "#     if col in data.columns:\n",
    "#         print(f\"\\nUnique values in '{col}': {data[col].nunique()}\")\n",
    "#         print(data[col].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd1f35a",
   "metadata": {},
   "source": [
    "### III. Handling Missing Values: \n",
    "Identify patterns of missing data (MCAR, MAR, MNAR). Then apply appro-priate imputation techniques (mean, median, mode, forward/backward fill, K-NN imputation). After that, compare the impact of different imputation strategies on data quality and distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1564eecc",
   "metadata": {},
   "source": [
    "### IV. Data Normalization: \n",
    "Apply Min-Max scaling, standardization (Z-score normalization), and robust scaling for data with outliers. Then compare distributions before and after normalization using appropriate visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc82e09",
   "metadata": {},
   "source": [
    "### V. Categorical Encoding: \n",
    "Identify categorical variables requiring encoding. Apply suitable encoding for each type of categorical variable, such as one-hot encoding for nominal variables and ordinal encoding for ordinal variables. Discuss strategies for handling high-cardinality categorical features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379878fe",
   "metadata": {},
   "source": [
    "### VI. Feature Selection: \n",
    "Choose a suitable feature selection method. For example, calculate the correlation matrix, use variance threshold, apply feature importance from tree-based models, or implement recursive feature elimination (RFE). Then compare the selected feature sets and justify your final selection."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
